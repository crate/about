# LiteLLM supports all models from Ollama.
# https://docs.litellm.ai/docs/providers/ollama
#
# uvx litellm --config etc/litellm-config.yaml --detailed_debug
#
# TODO: Use `ollama_chat`
#       We recommend using ollama_chat for better responses.
---

model_list:
  - model_name: "llama3.2"
    litellm_params:
      model: "ollama/llama3.2"
      api_base: "http://localhost:11434"

  - model_name: "gemma3:1b"
    litellm_params:
      model: "ollama/gemma3:1b"
      api_base: "http://localhost:11434"

  - model_name: "qwen3:0.6b"
    litellm_params:
      model: "ollama/qwen3:0.6b"
      api_base: "http://localhost:11434"

  - model_name: "deepseek-r1:7b"
    litellm_params:
      model: "ollama/deepseek-r1:7b"
      api_base: "http://localhost:11434"

# https://github.com/BerriAI/litellm/issues/1517#issuecomment-1922022209
#model_list:
#  - model_name: ollama-codellama
#    litellm_params:
#      model: ollama/codellama:70b
#     api_base: http://0.0.0.0:11434
#      rpm: 1440
#    model_info:
#      version: 2

#litellm_settings:
#  drop_params: True
#  set_verbose: True
